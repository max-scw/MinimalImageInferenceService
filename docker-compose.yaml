version: '3'

services:
   # ----- toy inference
  basler-camera-service:
    container_name: basler-api
    build:
      context: ./
      dockerfile: BaslerAPI.Dockerfile
    image: schwmax/basler-api
#    restart: unless-stopped
    environment:
      - TZ=Europe/Berlin
      # customize
      - TEST_IMAGE=/home/app/testimage.bmp
    volumes:
      - ./BaslerAPI/230221_143518_0000001264_CAM1_NORMAL_OK.bmp:/home/app/testimage.bmp
    ports:
      - 3956:3956
      - 46000:46000/udp  # set Streaming Port to fixed value with pylonViewer
      - 5005:5050  # external access port | can be closed

  model-backend:
    build:
      context: ./
      dockerfile: ModelBackend.Dockerfile
    image: schwmax/model-backend
    container_name: model-backend
    environment:
      - TZ=Europe/Berlin
      - MODEL_FILENAME=20240306_VIAB1-2_YOLOv7tiny.onnx
      - MODEL_IMAGE_SIZE=(544, 640)
      - MODEL_PRECISION=fp32
    volumes:
      - ./UIdata/20240306_VIAB1-2_YOLOv7tiny.onnx:/home/app/data/model.onnx
    ports:
      - 5007:5050  # external access port // only for debugging

  toy-inference:
    build:
      context: ./
      dockerfile: InferenceUI.Dockerfile
    image: schwmax/inference-ui
    container_name: toy-inference
#    restart: unless-stopped
    environment:
      - TZ=Europe/Berlin
      # variables to customize container
#      - PREFIX=TI
      - TI_TITLE="MVE for object recognition"
      - TI_DESCRIPTION="loreipsum ..."
      # model
      - TI_MODEL_FILENAME=20240306_VIAB1-2_YOLOv7tiny.onnx
#      - TI_MODEL_CLASS_MAP=
      - TI_MODEL_COLOR_MAP=colors.txt
      - TI_MODEL_IMAGE_SIZE=(544, 640)
      - TI_MODEL_PRECISION=fp32
      # camera
#      - TI_CAMERA_URL=http://basler-api:5050/basler/take-photo
      - TI_CAMERA_URL=http://basler-api:5050/test/image
#      - TI_CAMERA_SERIAL_NUMBER
#      - TI_CAMERA_IP_ADDRESS
#      - TI_CAMERA_TIMEOUT
#      - TI_CAMERA_TRANSMISSION_TYPE
#      - TI_CAMERA_DESTINATION_IP_ADDRESS
#      - TI_CAMERA_DESTINATION_PORT
      - TI_CAMERA_EMULATE_CAMERA=True
      # impress
      - TI_IMPRESS_PROJECT_NAME="MVE inference UI test"
#      - TI_IMPRESS_AUTHOR
#      - TI_IMPRESS_STATUS
#      - TI_IMPRESS_ADDITIONAL_INFO
#      - TI_IMPRESS_PROJECT_LINK=https://github.com/max-scw/MinimalInferenceUI.git

    volumes:
      # mount model into container
#      - ./UIdata/20240306_VIAB1-2_YOLOv7tiny.onnx:/home/app/model.onnx
      - ./UIdata/:/home/app/data/
    ports:
      - 5006:8501  # external access port

